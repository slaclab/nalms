{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Next ALarM Sytem","text":"<p>NALMS is an alarm system application designed for availability, integrability, and extensibility. The NALMS development was driven by SLAC's efforts to replace the Alarm Handler, due for deprecation as a Motif-based application, and to introduce process improvements addressing hierarchy implementation overhead, limited operator engagement, and operator display integration.</p>"},{"location":"#docker","title":"Docker","text":"<p>This repository is packaged with tools for Docker based deployment. There are several reasons containerization is an advantageous:  </p> <ul> <li>The Kafka brokers may be straighforwardly deployed and the cluster scaled. Configurations are therefore transferable and port exposures may be configured directly on the Docker deployment.</li> <li>Contained applications may run in parallel, facilitating blue/green deployment workflows. </li> </ul> <p>This docker application consists of the following containers:</p> <ul> <li>Zookeeper</li> <li>Kafka</li> <li>Phoebus Alarm Server</li> <li>Phoebus Alarm Logger</li> <li>Elasticsearch</li> <li>Grafana</li> <li>An Example IOC</li> <li>Cruise Control</li> </ul> <p>Docker-compose may be used to run a packaged example with all components.</p> <pre><code>$ docker-compose up\n</code></pre> <p>Once running, the cruise control dashboard is available at http://localhost:9090, and the grafana alarm log dashboard is available at http://localhost:3000. The alarm logger may need to be restarted if using docker compose, due to a slight delay in the alarm server startup. </p> <p>Operations on the IOC for this demo can be performed by running caputs/cagets after attaching to the running container. </p>"},{"location":"SLAC/","title":"SLAC","text":""},{"location":"SLAC/#development","title":"Development","text":"<p>On aird-b50-srv01, source the development environment:</p> <pre><code>$ source ${PACKAGE_TOP}/nalms/setup/aird-b50-srv01/dev.env\n</code></pre>"},{"location":"cli/","title":"Command Line Tools","text":"<p>In order to abstract the deployment process and interact with the Dockerized applications, a CLI has been packaged with the NALMS repository. A properly configured environment will define the following NALMS specific variables. EPICS variables must also be defined for appropriate networking. The <code>nalms-tools</code> package must be installed into the active python environment for </p> Variable Description NALMS_KAFKA_PROPERTIES Path to Kafka properties file NALMS_ZOOKEEPER_PORT Zookeeper port NALMS_ES_PORT Elasticsearch port NALMS_CRUISE_CONTROL_PORT Cruise control port NALMS_KAFKA_PORT Exposed broker port NALMS_GRAFANA_PORT Grafana port NALMS_ES_HOST Elasticsearch network host address NALMS_KAFKA_BOOTSTRAP Bootstrap node for Kafka connections NALMS_ZOOKEEPER_HOST Zookeeper network host address NALMS_KAFKA_HOST Kafka broker network host address NALMS_CONFIGURATIONS Comma separated list of configurations for launching Grafana NALMS_HOME Path to NALMS repository NALMS_CLIENT_JAR Path to NALMS client jar file NALMS_ALARM_SERVER_PROPERTIES Path to alarm server properties file NALMS_ALARM_LOGGER_PROPERTIES Path to alarm logger properties file NALMS_CRUISE_CONTROL_PROPERTIES Path to cruise control properties file NALMS_GRAFANA_DASHBOARD_DIR Path to Grafana dashboard directory NALMS_GRAFANA_CONFIG Path to Grafana configuration file NALMS_GRAFANA_DATASOURCE_FILE Path to Grafana datasource file NALMS_ZOOKEEPER_CONFIG Path to Zookeeper configuration file NALMS_ES_CONFIG Path to elasticsearch configuration file <p>For a multi-broker deployment, ports should be set on a per broker basis, divying the configuration script into separate scripts for each broker. </p>"},{"location":"cli/#convert-alh","title":"convert-alh","text":"<p>Command converts ALH configuration to Phoebus XML representation.</p> <pre><code>$ bash cli/nalms convert-alh alh_file output_filename config_name\n</code></pre>"},{"location":"cli/#delete-configuration","title":"delete-configuration","text":"<p>Delete Kafka topics associated with a configuration.</p> <pre><code>$ bash cli/nalms delete-configuration configuration_name\n</code></pre>"},{"location":"cli/#generate-kafka-certs","title":"generate-kafka-certs","text":"<p>Generate certificates for Kafka trust store for configuration with SSL.</p> <pre><code>$ bash cli/nalms generate-kafka-certs domain password\n</code></pre>"},{"location":"cli/#launch-editor","title":"launch-editor","text":"<p>Launch the configuration editor.</p> <pre><code>$ bash cli/nalms launch-editor\n</code></pre>"},{"location":"cli/#list-configurations","title":"list-configurations","text":"<p>List active Kafka configurations.</p> <pre><code>$ bash cli/nalms list-configurations\n</code></pre>"},{"location":"cli/#start-alarm-logger","title":"start-alarm-logger","text":"<p>Start the alarm logger for a given configuration name. This will create an image named <code>nalms-logger-${CONFIG_NAME}</code>.</p> <pre><code>$ bash cli/nalms start-alarm-logger config_name\n</code></pre>"},{"location":"cli/#start-alarm-server","title":"start-alarm-server","text":"<p>Start the alarm server for a given configuration name and configuration file. Configuration name must match that defined in the configuration file. This will create an image named <code>nalms-server-${CONFIG_NAME}</code>.</p> <pre><code>$ bash cli/nalms start-alarm-server config_name config_file\n</code></pre>"},{"location":"cli/#start-cruise-control","title":"start-cruise-control","text":"<p>Start Cruise Control. This will create an image named <code>nalms_cruise_control</code>.</p> <pre><code>$ bash cli/nalms start-cruise-control\n</code></pre>"},{"location":"cli/#start-grafana","title":"start-grafana","text":"<p>Start the Grafana server, creating an image named <code>nalms_grafana</code>.</p> <pre><code>$ bash cli/nalms start-grafana\n</code></pre>"},{"location":"cli/#start-kafka-broker","title":"start-kafka-broker","text":"<p>Start a Kafak broker. Must indicate a broker number for broker identification. This will create an image named <code>nalms_kafka_$BROKER_NUMBER</code>.</p> <pre><code>$ bash cli/nalms start-kafka-broker --broker_number broker_number\n</code></pre>"},{"location":"cli/#start-phoebus-client","title":"start-phoebus-client","text":"<p>Launch the Phoebus client.</p> <pre><code>$ bash cli/nalms start-phoebus-client </code></pre>"},{"location":"cli/#start-zookeeper","title":"start-zookeeper","text":"<p>Start Zookeeper, creating image named <code>nalms_zookeeper</code>.</p> <pre><code>$ bash cli/nalms start-zookeeper\n</code></pre>"},{"location":"cli/#add-grafana-datasource","title":"add-grafana-datasource","text":"<p>Add a Grafana datasource for a configuration(s).</p> <pre><code>$ bash cli/nalms add-grafana-datasource config_names\n</code></pre>"},{"location":"cli/#build-grafana-dashboard","title":"build-grafana-dashboard","text":"<p>Build a Grafana dashboard for a configuration</p> <pre><code>$ bash cli/nalms build-grafana-dashboard config_name\n</code></pre>"},{"location":"cli/#build-alarm-ioc","title":"build-alarm-ioc","text":"<p>Build alarm ioc files</p> <pre><code>$ bash cli/nalms build-alarm-ioc app_name ioc_name config_name config_file target_architecture\n</code></pre>"},{"location":"components/","title":"Components","text":"<p>The NALMS alarm system is heavily modularized and consists of:</p> <ul> <li>Zookeeper for Kafka  orchestration</li> <li>Kafka brokers for robust state management</li> <li>Cruise control for Kafka cluster administration</li> <li>Phoebus alarm server for the translation of EPICS alarm events into Kafka messages</li> <li>Phoebus alarm logger for the translation of Kafka messages into Elasticsearch documents</li> <li>A Grafana log dashboard for Elasticsearch integration</li> <li>PyDM tools for GUI integration</li> <li>AlarmIOC for exposure of alarm state to control system</li> </ul> <p>Due to this modularization, the system is easily extensible and all alarm state, configuration actions, and alarm actions are exposed over the Elasticsearch server API or via a Kafka consumer, easily integrable with applications beyond the scope of this project.</p> <p></p>"},{"location":"configuration/","title":"Configuration","text":"<p>Alarm configurations are XML files organized with component (group), and PV tags. Component tags accept specifications for guidance, display, commands, and automated actions. Configuration options for groups are defined below:</p> Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. <p>PV tags accept specifications for enabling, latching, annunciating, description, delay, commands, associated displays, guidance, alarm count, filter, and automated actions. A configuration schema is provided here.</p> Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. description Text displayed in the alarm table when the alarm is triggered delay Alarm will be triggered if the PV remains in alarm for at least this time enabled If false, ignore the value of this PV latching Alarms will latch to the highest severity until the alarm is acknowledged and cleared. If false, alarm may recover without requiring acknowledgment count If the trigger PV exhibits a not-OK alarm severity for more than \u2018count\u2019 times within the alarm delay, recognize the alarm filter An optional expression that can enable the alarm based on other PVs. <p></p>"},{"location":"configuration/#alarm-configuration-editor-tool","title":"Alarm Configuration Editor Tool","text":"<p>The alarm configuration editor is be a PyQt tool for designing the alarm configuration XML files for use with the Phoebus alarm server as outlined in Section 3.2. Alternatively, any XML editor may be used to build the document directly. The editor has the following features:\u202f  * Ability to edit alarm hierarchy, create new groups and new PVs  * Ability to define all configuration items * Optional conversion and import of legacy ALH files </p> <p>Requirements for running the editor are given in the <code>environment.yml</code> file bundled with the NALMS package. This environment can be created with conda using:</p> <pre><code>$ conda env create -f environment.yml\n</code></pre> <p>And subsequently activated:</p> <pre><code>$ conda activate nalms\n</code></pre> <p>If choosing to build your own environment without conda, the requirements follow:   - python =3.8   - treelib   - lxml   - pyqt5   - kafka-python   - pydm</p> <p>PyDM dependence will eventually be dropped.   </p> <p>To launch the editor run:</p> <pre><code>$ bash cli/nalms launch-editor\n</code></pre>"},{"location":"demo/","title":"Demo","text":"<p>The following PV tree will be used for the demo:</p> <p></p> <p>We represent this tree with the configuration file in <code>examples/demo/demo.xml</code>.</p> <pre><code>&lt;?xml version='1.0' encoding='utf8'?&gt;\n&lt;config name=\"Demo\"&gt;\n&lt;component name=\"GROUP1\"&gt;\n&lt;pv name=\"DEMO:PV1\"&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;filter&gt;DEMO:PV2 &gt; 10&lt;/filter&gt;\n&lt;/pv&gt;\n&lt;pv name=\"DEMO:PV2\"&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/pv&gt;\n&lt;pv name=\"DEMO:PV3\"&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/pv&gt;\n&lt;/component&gt;\n&lt;component name=\"GROUP2\"&gt;\n&lt;component name=\"GROUP3\"&gt;\n&lt;pv name=\"DEMO:PV4\"&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/pv&gt;\n&lt;pv name=\"DEMO:PV5\"&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/pv&gt;\n&lt;/component&gt;\n&lt;pv name=\"DEMO:PV6\"&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/pv&gt;\n&lt;pv name=\"DEMO:PV7\"&gt;\n&lt;enabled&gt;true&lt;/enabled&gt;\n&lt;/pv&gt;\n&lt;/component&gt;\n&lt;/config&gt;\n</code></pre> <p>Notice that <code>DEMO:PV1</code> has an enabling filter based on the value of <code>DEMO:PV2</code>. This results in a disabled <code>DEMO:PV1</code> for values of <code>DEMO:PV2</code> less than or equal to ten.</p> <p>This demo is intended for running on SLAC's RHEL7 dev server; however, this same demo can be executed on machines provided that an apropriate environment is sourced. NALMS uses named Docker containers and so this demo cannot be run if the existing demo containers are running. The commands below are run using the existing RHEL7 docker installation. Users must be added to the docker group in order to interact with the containers. </p> <p>For client use: $NALMS_CLIENT_JAR must be defined as well as $NALMS_HOME. The client launch script creates a templated configuration file for the client from a template provided in $NALMS_HOME. </p>"},{"location":"demo/#if-running","title":"If running...","text":"<p>If the service containers have already been deployed, you can access cruise control at: http://localhost:9090, the Grafana instance at: http://localhost:3000, and launch the client using:</p> <pre><code>$ source /afs/slac/g/lcls/tools/script/ENVS64.bash\n$ source ${PACKAGE_TOP}/nalms/setup/aird-b50-srv01/demo.env\n$ nalms start-phoebus-client Demo\n</code></pre>"},{"location":"demo/#from-scratch","title":"From scratch","text":"<p>During this demo, we set up all services using the package CLI and the Docker images. On aird-b50-srv01, this can be sourced using: </p> <pre><code>$ source /afs/slac/g/lcls/tools/script/ENVS64.bash\n$ source ${PACKAGE_TOP}/nalms/setup/aird-b50-srv01/demo.env\n</code></pre>"},{"location":"demo/#demo-ioc","title":"Demo IOC","text":"<p>Start the demo ioc:</p> <pre><code>$ tmux new -s demo-ioc\n$ softIoc -d ${NALMS_HOME}/examples/demo/demo.db </code></pre> <p>Exit the tmux window using: <code>Ctr + b + d</code></p>"},{"location":"demo/#alarm-ioc","title":"Alarm IOC","text":"<p>For integration with edm/pydm displays etc., we can use the nalms cli to generate an alarm ioc for a given configuration. In this demo, we will use application name <code>nalmsDemo</code>, IOC name <code>demo</code>.</p> <pre><code>$ source $EPICS_SETUP/epicsenv-7.0.3.1-1.0.bash\n$ nalms build-alarm-ioc nalmsDemo demo Demo ${NALMS_HOME}/examples/demo/demo.xml rhel7-x86_64\n$ cd nalmsDemo\n$ make\n$ cd iocBoot/iocdemo\n$ tmux new -s demoAlarmIOC # run in a new window\n$ ../../bin/rhel7-x86_64/nalmsDemo st.cmd\n$ dbl\n</code></pre> <p>Exit the tmux window using: <code>Ctr + b + d</code></p>"},{"location":"demo/#kafka-cluster","title":"Kafka Cluster","text":"<p>Set up Kafka cluster (from repo root): </p> <pre><code>$ nalms start-zookeeper \n$ nalms start-kafka-broker --broker 0\n</code></pre>"},{"location":"demo/#cruise-control","title":"Cruise Control","text":"<p>Start cruise-control:</p> <pre><code>$ nalms start-cruise-control\n</code></pre> <p>Navigate to http://localhost:9090 to view the Cruise Control interface and monitors of the Kafka cluster. </p>"},{"location":"demo/#phoebus-alarm-server","title":"Phoebus Alarm Server","text":"<p>Start the Phoebus alarm server: (Note: launch requires the absolute path of the configuration file for docker volume mount)</p> <pre><code>$ nalms start-alarm-server Demo ${NALMS_HOME}/examples/demo/demo.xml --alarmioc true\n</code></pre>"},{"location":"demo/#elasticsearch","title":"Elasticsearch","text":"<p>Next, start the Elasticsearch service: </p> <pre><code>$ nalms start-elasticsearch\n</code></pre>"},{"location":"demo/#phoebus-alarm-logger","title":"Phoebus Alarm Logger","text":"<p>Wait at least a minute before starting the Phoebus alarm logger. The templates for the indices must be created before starting. Start the Phoebus alarm logger:</p> <pre><code>$ nalms start-alarm-logger Demo ${NALMS_HOME}/examples/demo/demo.xml\n</code></pre>"},{"location":"demo/#grafana","title":"Grafana","text":"<p>Now we'll add the configuration to be handled with Grafana. Add the Grafana datasource to the file:</p> <pre><code>$ nalms add-grafana-datasource Demo\n</code></pre> <p>This appended the datasource to the file <code>datasource.yml</code>. Now, create the Grafana dashboard:</p> <pre><code>$ nalms build-grafana-dashboard Demo\n</code></pre> <p>This created a dashboard for the Demo configuration in $NALMS_GRAFANA_DASHBOARD_DIR. Launch the Grafana instance:</p> <pre><code>$ nalms start-grafana\n</code></pre> <p>Launch firefox and navigate to http://localhost:3000. Select AlarmLogs from the available dashboards.</p>"},{"location":"demo/#phoebus-client","title":"Phoebus Client","text":"<p>Launch the Phoebus client</p> <pre><code>$ nalms start-phoebus-client Demo\n</code></pre> <p>Navigate to <code>Applications &gt; Alarm &gt; Alarm Tree</code> to view the process variable values. Navigate to <code>Applications &gt; Alarm &gt; Alarm Log</code></p>"},{"location":"demo/#inspect","title":"Inspect","text":"<p>To inspect the Docker containers run:</p> <pre><code>$ docker ps # to list container ids\n$ docker stats {CONTAINER_ID}\n</code></pre>"},{"location":"demo/#cleanup","title":"Cleanup","text":"<p>All containers may be stopped using the ids listed with:</p> <pre><code>$ docker ps\n$ docker stop {containter_id}\n</code></pre> <p>Remove lingering containers...</p> <pre><code> $ docker container rm {container_id}\n</code></pre> <p>You can access and exit the demo ioc by attaching to the tmux session:</p> <pre><code>$ tmux attach -t demo-ioc\n</code></pre> <p>Exiting:</p> <pre><code>$ &gt; exit\n$ exit\n</code></pre> <p>And the alarm ioc:</p> <pre><code>$ tmux attach -t demoAlarmIoc\n</code></pre> <p>Exiting:</p> <pre><code>$ &gt; exit\n$ exit\n</code></pre>"},{"location":"development/","title":"Docs","text":"<p>This project uses mkdocs for generating documentation. This can be installed with Python. Once mkdocs and mkdocs-material have been installed, the documentation may be served locally using the command:</p> <pre><code>$ mkdocs serve\n</code></pre> <p>A GitHub action workflow has been configured such that the docs are automatically created on merge to the <code>slaclab/nalms</code> <code>main</code> branch.</p>"},{"location":"development/#docker-images","title":"Docker images","text":"<p>Significant simplifications might be made to these docker images (moving to more modern OS etc.); however, I've tried to replicate the RHEL 7 design requirement as closely as possible to demonstrate the installation outlined in the design document. </p> <p>Newer versioned releases should be indicated to the nalms package by updates to version environment variables:</p> <pre><code># versions\nexport NALMS_DOCKER_ES_VERSION=v1.1\nexport NALMS_DOCKER_GRAFANA_VERSION=v1.1\nexport NALMS_DOCKER_ALARM_SERVER_VERSION=v1.1\nexport NALMS_DOCKER_ALARM_LOGGER_VERSION=v1.1\nexport NALMS_DOCKER_ZOOKEEPER_VERSION=v1.1\nexport NALMS_DOCKER_CRUISE_CONTROL_VERSION=v1.1\nexport NALMS_DOCKER_KAFKA_VERSION=v1.1\n</code></pre>"},{"location":"development/#useful-commands","title":"Useful commands:","text":"<p>To see all running and stopped containers</p> <pre><code>$ sudo docker container ls -a\n</code></pre> <p>The current installation of Docker on rhel... requires sudo for use</p> <p>to get the id of a running container:</p> <pre><code>$ sudo docker container ls\n</code></pre> <p>To get memory, CPU use:</p> <pre><code>sudo docker container stats ${CONTAINER_ID}\n</code></pre> <p>Container information, including networking info,  is available using:</p> <pre><code>$ sudo docker container inspect${CONTAINER_ID}\n</code></pre> <p>To attach to a running container:</p> <pre><code>$ sudo docker container exec -it fdc6ce9ce655 /bin/bash\n</code></pre>"},{"location":"development/#dockerhub-deployment","title":"DockerHub deployment","text":"<p>All Dockerhub images are hosted on the TID Advanced Controls Systems account (tidacs). A Github action has been defined for the automatic build of images on tagged releases to the main slaclab/master branch.</p> <p>All images should be eventually removed from this repository and moved into their own for convenient versioning.</p>"},{"location":"development/#ongoing-projects","title":"Ongoing projects","text":"<p>An attempt has been made to document development needs using Github projects here.</p>"},{"location":"development/#helpful-debugging","title":"Helpful debugging","text":""},{"location":"development/#phoebus-alarm-server","title":"Phoebus Alarm Server","text":"<p>In the event of problematic IOC connectivity, it may be worthwhile to connect to the Phoebus Alarm Server docker container using:</p> <pre><code>$ sudo docker container exec -it fdc6ce9ce655 /bin/bash\n</code></pre> <p>and edit the logging options in $LOGGING_CONFIG_FILE. </p> <pre><code>org.phoebus.applications.alarm.level = INFO\ncom.cosylab.epics.caj.level = FINE # handles connectivity\norg.phoebus.framework.rdb.level = WARNING\norg.phoebus.pv.level = FINE\norg.apache.kafka.level = SEVERE\n</code></pre>"},{"location":"development/#grafana-template","title":"Grafana template","text":"<p>In order to use the Grafana dashboard with a scaling number of configurations, a json template is located in <code>grafana/dashboards/alarm_logs_dashboard.json</code>. </p> <p>For further development of the dashboard, the template must be changed using a local Grafana instance. Steps for updating are: * Copy json representation * Remove id from the json representation * Replace datasource name entries from json representation with environment var placeholder $DATASOURCE_NAME </p>"},{"location":"epics_integration/","title":"EPICS integration","text":"<p>This repository is currently outfitted to integrate directly with SLAC's alarm IOC as used in conjunction with the legacy Alarm Handler. This is critical for a seamless transition between the ALH and NALMS system and will allow for the surfacing of EDM/PyDM alarming indicators (bypass markers) while using the new system. In addition to the bypass indicator, an acknowledgment indicator has also been added to the NALMS alarm IOC, allowing acknowledgment status to be indicated in displays just as bypasses. Postfixes are applied to the end of pv names to create new these new indicator pvs.</p> <p>The alarm ioc consists of:</p> <ul> <li>FP postifixed variables for indicating bypass, written to by system</li> <li>SV posftixed variables for scanning bypass indicator</li> <li>DP postfixed variables for propogating the alarm system's effective PV severity</li> <li>Group level STATSUMY postfixed variables for the propogation of the alarm system's effective group severity </li> <li>Group level  STATSUMYFP postfixed variables for the propogation of the alarm system's effective bypass status</li> </ul> <p>Inside the <code>nalms-phoebus-alarm-server</code> Docker image, the script: <code>phoebus-alarm-server/scripts/update-ioc.py</code> continually runs and updates the alarm ioc with the pvs' bypass states. EPICS environment variables may be passed to the image to configure EPICS.</p>"},{"location":"epics_integration/#generation","title":"Generation","text":"<p>Constructed using a template found in <code>nalms-tools/..</code></p>"},{"location":"install/","title":"Installation","text":"<p>The NALMS system has been written for deployment as a set of Docker images, allowing for the distribution of services across network hosts configurable via environment variables and mounted volumes. In the case that containers aren't favorable, the Dockerfiles inside this repository should serve as a guide for these deployments and component source materials should be consulted for installation details.</p> <p>The current NALMS iteration consists of the following Dockerhub hosted containers:</p>"},{"location":"install/#tidacsnalms-zookeeper","title":"tidacs/nalms-zookeeper","text":"<ul> <li>Zookeeper 3.5.9</li> </ul>"},{"location":"install/#tidacsnalms-kafka","title":"tidacs/nalms-kafka","text":"<ul> <li>Kafka 2.8.1 configured with cruise-control metrics reporter</li> <li>SSL configurable (see networking)</li> </ul>"},{"location":"install/#tidacsnalms-phoebus-alarm-server","title":"tidacs/nalms-phoebus-alarm-server","text":"<ul> <li>Phoebus alarm server (built from HEAD of main branch)</li> <li>Python script for monitoring Kafka topics updating alarm IOC with bypasses and acknowledgments</li> </ul>"},{"location":"install/#tidacsnalms-phoebus-alarm-logger","title":"tidacs/nalms-phoebus-alarm-logger","text":"<ul> <li>Phoebus alarm logger (built from HEAD of main branch)</li> </ul>"},{"location":"install/#tidacsnalms-elasticsearch","title":"tidacs/nalms-elasticsearch","text":"<ul> <li>Elasticsearch service (6.8.22)</li> <li>Script for templating of Alarm indices</li> </ul>"},{"location":"install/#tidacsnalms-grafana","title":"tidacs/nalms-grafana","text":"<ul> <li>Grafana service (7.5.3)</li> <li>Template Grafana dashboard for any configuration</li> <li>Can be launched with multiple configurations as a comma separated list</li> <li>Automatic generation of elasticsearch datasources based on network configs and configuration names</li> </ul>"},{"location":"install/#tidacsnalms-cruise-control","title":"tidacs/nalms-cruise-control","text":"<ul> <li>LinkendIn's Cruise Control monitor for Kafka clusters (built from HEAD of branch migrate_to_kafka_2_5, which is compatible with Kafka 2.7.0) </li> <li>Cruise Control web UI</li> </ul>"},{"location":"install/#download-images","title":"Download Images","text":"<p>Images may be downloaded from Dockerhub on a machine with Docker installed using the command (nalms-kafka here for example):</p> <pre><code>$ docker pull tidacs/nalms-kafka:latest\n</code></pre>"},{"location":"install/#client-installation","title":"Client installation","text":"<p>At present, we will build the client from the HEAD of the main branch of the Phoebus client hosted on Github.</p> <p>Requirements:</p> <ul> <li>OpenJDK == 11.0.2</li> <li>Maven == 3.6.0</li> <li>Git &gt;= 1.8 (likely compatable with lower, but only tested as low as 1.8.3)</li> </ul>"},{"location":"install/#installation_1","title":"Installation:","text":"<ol> <li> <p>Set <code>$JAVA_HOME</code>, <code>$MAVEN_HOME</code>, and <code>$NALMS_HOME</code>. Then update the path:   <code>$ export PATH=$JAVA_HOME/bin:$PATH   $ export PATH=$MAVEN_HOME/bin:$PATH</code>   Note: In afs, JAVA_HOME=${PACKAGE_TOP}/java/jdk-11.0.2, MAVEN_HOME=${PACKAGE_TOP}/maven/3.6.0, and NALMS_TOP=${PACKAGE_TOP}/nalms/current.</p> </li> <li> <p><code>cd</code> into installation directory</p> </li> <li>Get Phoebus repository   <code>$ git clone https://github.com/ControlSystemStudio/phoebus.git   $ cd phoebus</code></li> <li>Now replace the existing product pom file with that packaged by NALMS:   <code>$ rm phoebus-product/pom.xml   $ mv $NALMS_HOME/phoebus-client/pom.xml phoebus-product/pom.xml</code></li> <li> <p>Install the Phoebus client:   <code>$ mvn install -pl phoebus-product -am</code></p> </li> <li> <p>Define $NALMS_CLIENT_JAR in appropriate environment file.</p> </li> </ol>"},{"location":"install/#deploy","title":"Deploy","text":"<p>After pulling the latest image, it is recommended to use the CLI for launching of each image as checks for necessary environment variables are built in to the interface. A full description of each image configuration is described here.</p> <p>Modify <code>cli/nalms</code> for execution:</p> <pre><code>$ chmod +x cli/nalms\n</code></pre>"},{"location":"install/#configuration","title":"Configuration","text":"<p>This sections includes an attempt to address configuration items, giving some insight to the service configuration within the Dockerized components and their Docker arguments.</p> <p>An attempt has been made to sufficiently abstract scripts for deployments based on environment variables and a full descriptions of a complete environment for deployment is giving in the CLI documentation.</p> <p>Below, each component is outline with respect to Docker configuration variables and configuration file structure. For a full resource of available configurations, the source documentation will be linked. </p>"},{"location":"install/#elasticsearch","title":"Elasticsearch","text":""},{"location":"install/#configuration_1","title":"Configuration","text":"<p>The Elasticsearch configuration consists of three main files:</p> <ul> <li>elasticsearch.yml for configuring Elasticsearch</li> <li>jvm.options for configuring Elasticsearch JVM settings</li> <li>log4j2.properties for configuring Elasticsearch logging</li> </ul> <p>A reference for elasticsearch configuration files can be found here.</p> <p>In order for the Elasticsearch fields to be properly formatted, a template matching the topic scheme must be posted to the server. These may be versioned and are automatically applied to newly created indices. The initial script for templating NALMS topics is hosted in <code>elasticsearch/scripts/create_alarm_template.sh</code>. This template has been taken from the Phoebus source examples.</p>"},{"location":"install/#docker","title":"Docker","text":"<p>The elasticsearch node may be configured using an exposed port, node specific variables, and Kafka networking variables. Because this is a single node deployment,  <code>single-node</code> deployment is used. Java options may be specifified using the <code>ES_JAVA_OPTS</code> variable. The Elasticsearch docker image also creates the appropriate elasticsearch template for the configuration messages. The configuration files must be mounted to <code>/usr/share/elasticsearch/config</code>.</p> <p>The following Docker run command will lauch an Elasticsearch node reachable on host machine port 9200.</p> <pre><code>$ docker run \\\n-e node.name=node01 \\\n-e cluster.name=es-cluster-7 \\\n-e discovery.type=single-node \\\n-e ES_JAVA_OPTS=\"-Xms128m -Xmx128m\" \\\n-e ES_HOST=localhost \\\n-e ES_PORT=9200 \\\n-v \"${NALMS_ES_CONFIG}:/usr/share/elasticsearch/config\" \\\n-p \"$NALMS_ES_PORT:9200\" \\\n--name nalms_elasticsearch \\\n-d tidacs/nalms-elasticsearch:latest\n</code></pre>"},{"location":"install/#zookeeper","title":"Zookeeper","text":""},{"location":"install/#configuration_2","title":"Configuration","text":"<p>At present, Zookeeper is launched using the default settings. For more sophisticated deployments, a configuration with mounted configuration files would be preferable. The configuration file is mounted to the Zookeeper container at runtime. A description of the zookeeper configuration may be found here.</p>"},{"location":"install/#docker_1","title":"Docker","text":"<p>The following command will run Zookeeper accessible on the host machine at port 2181:</p> <pre><code>$ docker run -p \"${NALMS_ZOOKEEPER_PORT}:2181\" -e ZOOKEEPER_CONFIG=/tmp/zoo.cfg \\\n-v \"${NALMS_ZOOKEEPER_CONFIG}:/tmp/zoo.cfg\" --name nalms_zookeeper \\\n-d tidacs/nalms-zookeeper:latest\n</code></pre>"},{"location":"install/#kafka","title":"Kafka","text":""},{"location":"install/#configuration_3","title":"Configuration","text":"<p>This file is used to configure general properties of a Kafka broker including replication settings and communications protocols. Listeners are defined with respect to configured protocols and binding ports. Advertised listeners are configured with respect to configured protocol and exposed ports. </p> <p>The <code>replication.factor</code> must be appropriately modified based off of the number of nodes in the deployment. A single broker deployment would require <code>replication.factor</code> set to 1. A cluster deployment can accomodate a larger replication factor across the cluster and this file must be modified for the purpose. </p> <p>Networking configurations for SSL/TLS configuration settings are described here.</p> <p>Also defined in this file is the reference to the zookeeper docker image resource:</p> <pre><code>zookeeper.connect=zookeeper:2181\n</code></pre> <p>Certain configurations options may be defined on the topic level. In <code>phoebus-alarm-server/cli/commands/create-kafka-indices</code>, state topics are created with partitions and replications dependent on the cluster settings. After initial creation, the Talk and Command topics are modified to use the deletion cleanup policy with set retention time. At present, the Talk command unused. The create-kafka-indices command is automatically executed during alarm server docker image startup.</p> <p>There are many other settings pertaining to the optimization of the cluster and must be determined by traffic demands. A full catalog of available configurations may be found in the documentation, here.</p>"},{"location":"install/#docker_2","title":"Docker","text":"<p>The Kafka broker images require the definition of Kafka networking variables, <code>KAFKA_ADVERTISED_LISTENERS</code>, <code>KAFKA_LISTENER_SECURITY_PROTOCOL_MAP</code>, <code>KAFKA_LISTENERS</code>, <code>ZOOKEEPER_CONNECT</code> and must be provided a numeric broker ID. The image must be provisioned with an 8g memory allocation. Additional optimizations may be performed using the Docker image configurations. A configuration file must be mounted to <code>/opt/kafka/server.properties</code> for the image, with properly formatted networking and replication numbers. An example server configuration is given in <code>examples/demo/config/server.properties</code>.</p> <p>An example run command for the Kafka docker image is given below:</p> <pre><code>$ docker run -m 8g  \\\n-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${HOST_IP}:9092,CONNECTIONS_FROM_HOST://${HOST_IP}:19092 \\\n-e KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT \\\n-e KAFKA_LISTENERS=PLAINTEXT://${HOST_IP}:9092,CONNECTIONS_FROM_HOST://0.0.0.0:19092 \\\n-e ZOOKEEPER_CONNECT=${HOST_IP}:2182 \\\n-e BROKER_ID=0 \\\n-v \"/full/path/to/examples/demo/config/server.properties:/opt/kafka/server.properties\" \\\n-p \"19092:19092\" \\\n--name nalms_kafka_0 \\\n-d tidacs/nalms-kafka:latest\n</code></pre> <p>Instructions on configuring the Docker image with SSL are given in networking.</p>"},{"location":"install/#phoebus-alarm-server","title":"Phoebus Alarm Server","text":""},{"location":"install/#configuration_4","title":"Configuration","text":"<p>The Phoebus alarm server configuration properties file defines the EPICS configuration and Elasticsearch host configuration. A full preference list can be found in the CS-Studio documentation.</p> <p>Of particular importance are the EPICS, Kakfa, and Elasticsearch properties. The Alarm Server requires:</p> <pre><code># Channel access settings\norg.phoebus.pv.ca/addr_list=localhost:5064\norg.phoebus.pv.ca/server_port=5064\norg.phoebus.pv.ca/repeater_port=5065\norg.phoebus.pv.ca/auto_addr_list=no\n\n# pvAccess settings\n\n# Kafka\norg.phoebus.applications.alarm/server=kafka:19092\n</code></pre>"},{"location":"install/#docker_3","title":"Docker","text":"<p>The Phoebus alarm server requires mounting of the configuration file with the Docker volume option and the definition of environment variables indicating Kafka networking address, whether the alarm IOC is to be used, and the EPICS configuration settings to access the alarm and variable iocs.  The image supports the substitution of networking variables ($KAFKA_BOOTSTRAP and EPICS variables). Alternatively, these can be defined directly in the configuration file.</p> <p>The Docker run command for the packaged example is given below:</p> <pre><code>$ docker run -v $CONFIG_FILE:/tmp/nalms/$CONFIG_NAME.xml \\\n--name nalms_server_$CONFIG_NAME \\\n-v \"${NALMS_ALARM_SERVER_PROPERTIES}:/opt/nalms/config/alarm_server.properties\" \\\n-e ALARM_IOC=false \\\n-e KAFKA_BOOTSTRAP=\"${NALMS_KAFKA_BOOTSTRAP}\" \\\n-e EPICS_CA_ADDR_LIST=\"${EPICS_CA_ADDR_LIST}\" \\\n-e EPICS_CA_SERVER_PORT=\"${EPICS_CA_SERVER_PORT}\" \\\n-e EPICS_CA_REPEATER_PORT=\"${EPICS_CA_REPEATER_PORT}\" \\\n-e EPICS_PVA_ADDR_LIST=\"${EPICS_PVA_ADDR_LIST}\" \\\n-e EPICS_PVA_SERVER_PORT=\"${EPICS_PVA_SERVER_PORT}\" \\\n-e EPICS_PVA_REPEATER_PORT=\"${EPICS_PVA_REPEATER_PORT}\" \\\n-e ALARM_SERVER_PROPERTIES=\"/opt/nalms/config/alarm_server.properties\" \\\n-d -t tidacs/nalms-phoebus-alarm-server:latest start-server $CONFIG_NAME /tmp/nalms/$CONFIG_NAME.xml\n</code></pre> <p>The configuration file must be mounted to `/tmp/nalms/${CONFIG_NAME}, for internal identification.</p>"},{"location":"install/#phoebus-alarm-logger","title":"Phoebus Alarm Logger","text":""},{"location":"install/#configuration_5","title":"Configuration","text":"<p>The alarm logger properties file requires the definition of Elasticsearch and Kafka networking environment variables. The templated file used by the image is hosted at <code>phoebus-alarm-logger/logger.properties</code>.  </p> <pre><code># location of elastic node/s\nes_host=localhost\nes_port=9200\n\n# Kafka server location\nbootstrap.servers=localhost:9092\n</code></pre> <p>Additionally, logging for the logger is configurable and defined in <code>phoebus-alarm-server/logger.properties</code>.  </p>"},{"location":"install/#docker_4","title":"Docker","text":"<p>The Phoebus alarm logger requires the mounting of the configuration file with the Docker volume option. The image supports the interpolation of networking variables $NALMS_ES_HOST, $NALMS_ES_PORT, and $NALMS_KAFKA_BOOTSTRAP in this file. The Docker run command for the packaged example is given below:</p> <pre><code>$ docker run -v $CONFIG_FILE:/tmp/nalms/$CONFIG_NAME.xml \\\n-e ES_HOST=\"${NALMS_ES_HOST}\" \\\n-e ES_PORT=\"${NALMS_ES_PORT}\" \\\n-e BOOTSTRAP_SERVERS=\"${NALMS_KAFKA_BOOTSTRAP}\" \\\n-e ALARM_LOGGER_PROPERTIES=\"/opt/nalms/config/alarm_logger.properties\" \\\n-v \"${ALARM_LOGGER_PROPERTIES}:/opt/nalms/config/alarm_logger.properties\" \\\n--name nalms_logger_$CONFIG_NAME \\\n-d tidacs/nalms-phoebus-alarm-logger:latest start-logger $CONFIG_NAME /tmp/nalms/$CONFIG_NAME.xml\n</code></pre> <p>The configuration file must be mounted to `/tmp/nalms/${CONFIG_NAME}, for internal identification.</p>"},{"location":"install/#phoebus-client","title":"Phoebus Client","text":""},{"location":"install/#configuration_6","title":"Configuration","text":"<p>Like the alarm server and logger, the client also accepts a properties file that defines networking: here.</p>"},{"location":"install/#grafana","title":"Grafana","text":""},{"location":"install/#configuration_7","title":"Configuration","text":"<p>Grafana datasources and dashboards may be programatically provisioned as outlined here. Elasticsearch datasources define an index and networking variables. </p> <p>General Grafana configuration is described here.</p> <p>The dashboard template is hosted at <code>grafana/dashboards/alarm_logs_dashboard.json</code> and a configuration dashboard can be created using the <code>cli/nalms build-grafana-dashboard config-name</code> command. The datasource may be added to an existing datasource file using the  <code>cli/nalms add-grafana-datasource config-name</code> command or manually created.</p>"},{"location":"install/#docker_5","title":"Docker","text":"<p>The Grafana image requires mounting of the dashboards, datasource file, and configuration file. The Docker run command for the packaged example is given below:</p> <pre><code>$ docker run \\\n-p \"${NALMS_GRAFANA_PORT}:3000\" \\\n-v \"${NALMS_GRAFANA_DASHBOARD_DIR}:/var/lib/grafana/dashboards\" \\\n-v \"${NALMS_GRAFANA_DATASOURCE_FILE}:/etc/grafana/provisioning/datasources/all.yml\" \\\n-v \"${NALMS_GRAFANA_CONFIG}:/etc/grafana/config.ini\" \\\n-e ES_HOST=$NALMS_ES_HOST \\\n-e ES_PORT=$NALMS_ES_PORT \\\n--name nalms_grafana \\\n-d tidacs/nalms-grafana:latest\n</code></pre> <p>The datasource file must be mounted to <code>/etc/grafana/provisioning/datasources/all.yml</code>, the dashboard directory must be mounted to <code>/var/lib/grafana/dashboards</code>, and the configuration must be mounted to <code>/etc/grafana/provisioning/datasources/all.yml</code>. The Grafana dashboards are then reachable at localhost:${NALMS_GRAFANA_PORT} in browser.</p>"},{"location":"install/#cruise-control","title":"Cruise Control","text":""},{"location":"install/#configuration_8","title":"Configuration","text":"<p>The <code>cruise-control/cruisecontrol.properties</code> file dictates the behavior of the cruise control server, allowing definition of relevant thresholds and networking nodes. The <code>tidacs/nalms-cruise-control</code> image performs interpolation on this file in order to pass the relevant environment variables. </p> <p>See wiki: https://github.com/linkedin/cruise-control/wiki https://github.com/linkedin/cruise-control-ui/wiki/Single-Kafka-Cluster</p>"},{"location":"install/#docker_6","title":"Docker","text":"<p>In order to run this image, you must mount a cruisecontrol.properties to a path specified with the $CRUISE_CONTROL_PROPERTIES env variable. The image will perform interpolation on properties files with $BOOTSTRAP_SERVERS or $ZOOKEEPER_CONNECT as placeholders and defined $BOOTSTRAP_SERVERS or $ZOOKEEPER_CONNECT environment variables. The Docker run command for the packaged example is given below:</p> <pre><code>$ docker run \\\n-e BOOTSTRAP_SERVERS=\"${NALMS_KAFKA_BOOTSTRAP}\" \\\n-e ZOOKEEPER_CONNECT=\"${NALMS_ZOOKEEPER_HOST}:${NALMS_ZOOKEEPER_PORT}\" \\\n-e CRUISE_CONTROL_PROPERTIES=\"/opt/cruise-control/config/cruisecontrol.properties\" \\\n-v \"${NALMS_CRUISE_CONTROL_PROPERTIES}:/opt/cruise-control/config/cruisecontrol.properties\" \\\n--name nalms_cruise_control \\\n-p \"$NALMS_CRUISE_CONTROL_PORT:9090\" -d tidacs/nalms-cruise-control:latest\n</code></pre> <p>The Cruise Control UI is then available in browser at localhost:9090.</p>"},{"location":"kafka/","title":"Kafka","text":"<p>Apache Kafka will be used to maintain alarm state messages, alarm hierarchy configuration, and for the communication of commands between the client and alarm server.  </p> <p>Kafka uses a publish/subscribe model to synchronize state between data producers and data ingestors. Messages are constructed using a key-value structure. Producers write events to Kafka servers (brokers) using categorized messages termed topics. Topic messages are divided into a designated number of bucketed partitions, defined using the message keys. These partitions are replicated across brokers, with one replica elected as the leader responsible for the reading/writing of a topic. In the case of leader broker failure, a replica becomes leader. Leaders write new messages to other replicas and reads/writes are consequently parallelized across the cluster.  </p> <p>Kafka brokers are synchronized by a Zookeeper node. This Zookeeper node is responsible for leader election, maintaining a registry of cluster members, configuring topics (number of partitions, leader location, etc.), and access control. We will use the Zookeeper packaged with the Kafka distribution until its deprecation. The Kafka broker configuration file assigns an id for the broker, communication protocols, partition count, compaction settings and others.  </p> <p>The latest Kafka version as of writing (2.8.1) will be used out of the box, with configuration options tailored to frequent log compaction for state maintenance. The release of Kafka Improvement Proposal 500 at some point in 2021 warrants version reconsideration, as the new deprecation of zookeeper will allow the removal of the dedicated server and move to a self managed quorum model.</p> <p>The NALMS production Kafka cluster consists of three nodes, topic deletion enabled, with compaction cleanup policy for state messages and deletion for configuration and commands and frequent cleanup operations (max lag 1s). A development cluster consists of only a single node, hosted locally. A full description of configuration options is provided in the Apache Kafka documentation.</p> <p>Each broker may be configured with a keystore and truststore for SSL authentication and encryption. </p>"},{"location":"kafka/#kafka-messages","title":"Kafka messages","text":"<p>Categorized Kafka messages facilitate all interactions with the alarm server. Alarm events are translated into Kafka messages by the alarm server (state topic), commands are communicated to the alarm server (command topic), and configurations are defined and manipulated (config topic). The key-value structure of the Kafka messages maintains the alarm hierarchy. Keys are prefixed with \u201ccommand\u201d, \u201cstate\u201d, or \u201cconfig\u201d, and represent the full alarm item path as forward slash delineated locations in the hierarchy. </p> <p>For example, KLYS:LI23:21:DL_WG_TEMP in the following tree would be indicated by the path: /Temp/KLYS/KLYS:LI23:21/KLYS:LI23:21:DL_WG_TEMP.</p> <pre><code>Temp  \n\u2514\u2500\u2500 KLYS  \n    \u251c\u2500\u2500 KLYS:LI23:11  \n    \u2502   \u2514\u2500\u2500 KLYS:LI23:11:DL_WG_TEMP  \n    \u251c\u2500\u2500 KLYS:LI23:21  \n    \u2502   \u2514\u2500\u2500 KLYS:LI23:21:DL_WG_TEMP  \n    \u251c\u2500\u2500 KLYS:LI23:31  \n    \u2502   \u2514\u2500\u2500 KLYS:LI23:31:DL_WG_TEMP  \n    \u251c\u2500\u2500 KLYS:LI23:41  \n    \u2502   \u2514\u2500\u2500 KLYS:LI23:41:DL_WG_TEMP  \n    \u251c\u2500\u2500 KLYS:LI23:51\n    \u2502   \u2514\u2500\u2500 KLYS:LI23:51:DL_WG_TEMP\n    \u251c\u2500\u2500 KLYS:LI23:61\n    \u2502   \u2514\u2500\u2500 KLYS:LI23:61:DL_WG_TEMP\n    \u251c\u2500\u2500 KLYS:LI23:71\n    \u2502   \u2514\u2500\u2500 KLYS:LI23:71:DL_WG_TEMP\n    \u2514\u2500\u2500 KLYS:LI23:81\n        \u2514\u2500\u2500 KLYS:LI23:81:DL_WG_TEMP\n</code></pre> <p>The Kafka configuration message for the PV would be keyed by the string <code>config:/Temp/KLYS/KLYS:LI23:11/KLYS:LI23:11:DL_WG</code>. Associated values are JSON representations of the associated values. Representations for alarm tree leaves and nodes are outlined below. Undefined elements are omitted in practice.</p>"},{"location":"kafka/#alarm-leaf-configuration","title":"Alarm leaf configuration","text":"<pre><code>{\n\"user\":        String,\n\"host\":        String,\n\"description\": String,\n\"delay\":       Integer,\n\"count\":       Integer,\n\"filter\":      String,\n\"guidance\": [{\"title\": String, \"details\": String}],\n\"displays\": [{\"title\": String, \"details\": String}],\n\"commands\": [{\"title\": String, \"details\": String}],\n\"actions\":  [{\"title\": String, \"details\": String}]\n}\n</code></pre>"},{"location":"kafka/#alarm-node-configuration","title":"Alarm node configuration","text":"<pre><code>{\n\"user\":        String,\n\"host\":        String,\n\"guidance\": [{\"title\": String, \"details\": String}],\n\"displays\": [{\"title\": String, \"details\": String}],\n\"commands\": [{\"title\": String, \"details\": String}],\n\"actions\":  [{\"title\": String, \"details\": String}]\n}\n</code></pre>"},{"location":"kafka/#alarm-leaf-state","title":"Alarm leaf state","text":"<pre><code>{\n\"severity\": String,\n\"latch\": Boolean,\n\"message\":  String,\n\"value\":    String,\n\"time\": {\n\"seconds\": Long,\n\"nano\":    Long\n},\n\"current_severity\": String,\n\"current_message\":  String,\n\"mode\":     String,\n}\n</code></pre>"},{"location":"kafka/#alarm-node-state","title":"Alarm node state","text":"<pre><code>{\n\"severity\": String,\n\"mode\":     String,\n}\n</code></pre>"},{"location":"kafka/#command","title":"Command","text":"<pre><code>{\n\"user\":    String,\n\"host\":    String,\n\"command\": String\n}\n</code></pre>"},{"location":"legacy/","title":"Converting from ALH","text":"<p>The ALH -&gt; Phoebus Python (&gt;=3.7) conversion tool is handled by a package currently defined in the <code>nalms-tools</code> directory. </p> <pre><code>$ cd nalms-tools\n$ pip install -e .\n</code></pre> <p>The entry point console script is then available with:</p> <pre><code>$ convert-alh config_name input_filename output_filename\n</code></pre> <p>Several features of the ALH cannot be translated to Phoebus configurations and are deprecated in NALMS. These are the ALIAS, ACKPV, SEVRCOMMAND, STATCOMMAND, and BEEPSEVERITY ALH configuration entries. The conversion script will print any failures to STDOUT.</p> <p>At present, ALH inclusions will parsed and reserialized into a single Phoebus XML configuration file. Future CS-Studio development with include the ability to accomodate file inclusions within the tree structure such that nested files may be similarly structured.</p>"},{"location":"legacy/#subsystem-demo","title":"Subsystem demo","text":"<p>In this demo we will convert an existing ALH configuration file into a suitable Phoebus XML configuration file using the packaged nalms CLI. This demo is written to run using the development environment on aird-b50-srv01 and assumes already running kafka cluster, elasticsearch, and Grafana.</p> <p>Source the appropriate environment:</p> <pre><code>$ source ${PACKAGE_TOP}/nalms/setup/aird-b50-srv01/demo.env\n</code></pre> <p>Use the top level subsystem ALH config file to create the XML file:</p> <pre><code>$ nalms convert-alh ${TOOLS}/AlarmConfigsTop/mgnt/prod/lcls/alh/mgnt.alhConfig mgnt.xml Mgnt\n</code></pre> <p>Launch the Phoebus alarm server for the configuration:</p> <pre><code># must use full path to the configuration file\n$ nalms start-alarm-server Mgnt $(pwd)/mgnt.xml\n</code></pre> <p>Launch the Phoebus alarm logger for the configuration:</p> <pre><code># must use full path to the configuration file\n$ nalms start-alarm-logger Mgnt $(pwd)/mgnt.xml\n</code></pre> <p>Launch the client to view the alarm tree:</p> <pre><code>$ nalms start-phoebus-client Mgnt\n</code></pre> <p>Now, set up for use with Grafana. Add to the Grafana datasource:</p> <pre><code>$ nalms add-grafana-datasource Mgnt\n</code></pre> <p>This appended the datasource to the file $NALMS_GRAFANA_DATASOURCE_FILE. Now, create the Grafana dashboard:</p> <pre><code>$ nalms build-grafana-dashboard Mgnt\n</code></pre> <p>This created a dashboard for the Demo configuration in $NALMS_GRAFANA_DASHBOARD_DIR. Relaunch grafana:</p> <pre><code>$ docker container stop nalms_grafana\n$ docker container rm nalms_grafana\n$ nalms start-grafana\n</code></pre> <p>You will now see the dashboard available at the Grafana instance at http://localhost:3000.</p>"},{"location":"networking/","title":"Kafka SSL","text":"<p>The Kafka docker image defined in this repository has been configured to run with SSL enabled or not, indicated by the <code>USE_SSL=true</code> environment variable. For SSL use, it is necessary to mount a configuration file with the following relevant items defined in <code>server.properties</code> to the <code>/opt/kafka/server.properties</code>:</p> <pre><code>ssl.truststore.location=/opt/kafka/ssl/server.truststore.jks\nssl.keystore.location=/opt/kafka/ssl/server.keystore.jks\nsecurity.inter.broker.protocol=SSL\nssl.client.auth=requested\nssl.keystore.type=JKS\nssl.endpoint.identification.algorithm=\n</code></pre> <p>Additionally, the listener security protocol map defined in the environment variables must be reflect outgoing SSL messages. For example:</p> <pre><code>KAFKA_ADVERTISED_LISTENERS: SSL://kafka.broker1:9092,CONNECTIONS_FROM_HOST://localhost:19093\nKAFKA_LISTENER_SECURITY_PROTOCOL_MAP: SSL:SSL,CONNECTIONS_FROM_HOST:PLAINTEXT\nKAFKA_LISTENERS: SSL://kafka.broker1:9092,CONNECTIONS_FROM_HOST://0.0.0.0:19093\n</code></pre> <p>Relevant passwords must also be passed:</p> <pre><code>TRUSTSTORE_PASSWORD: kafkabroker\nKEYSTORE_PASSWORD: kafkabroker\nKEY_PASSWORD: kafkabroker\n</code></pre> <p>A utility script for generating the truststore/keystore can be run:</p> <pre><code>$ bash cli/nalms generate-kafka-certs domain password\n</code></pre> <p>This utility might be decomposed further into truststore/keystore/key passwords. The appropriate keystore will then be mounted to the docker volume at <code>/opt/kafka/ssl</code>. Keys for each broker will need to be added to the respective trust stores of each broker node.Documentation on SSL for Kafka may be found here.</p> <p>Instructions for configuring the Kafka truststore may be found here</p>"},{"location":"networking/#phoebus","title":"Phoebus","text":"<p>The Phoebus alarm server and logger to not accomodate SSL/TLS out of the box and will require development. The workflow that must be changed to accomodate SSL on the Phoebus side can be found in the following file:  <code>phoebus/app/alarm/model/src/main/java/org/phoebus/applications/alarm/client/KafkaHelper.java</code>. Logically, this will mean exposing the following additional streams settings to the application:</p> <pre><code>security.protocol=SSL\nssl.truststore.location=/path/to/kafka.client.truststore.jks\nssl.truststore.password=truststore_password\nssl.keystore.location=/path/to/kafka.client.keystore.jks\nssl.keystore.password=keystore_password\nssl.key.password=key_password\n</code></pre> <p>More information for setting up these settings may be found here.</p>"},{"location":"networking/#elasticsearch","title":"Elasticsearch","text":"<p>Instructions for configuring elasticsearch security may be found here: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/ssl-tls.html</p> <p>The Docker image provided with this repository is based off of the official Elasticsearch 6.8 image and the following guide can be used to configure SSL/TLS with this image: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/docker.html</p>"},{"location":"networking/#grafana","title":"Grafana","text":"<p>Grafana Elasticsearch datasources may be configured to use certificates during setup. Options for provisioning datasources may be found here: https://grafana.com/docs/grafana/latest/administration/provisioning/</p>"},{"location":"networking/#pydm","title":"PyDM","text":"<p>The PyDM datasource and client widgets will need to be built to accomodate authentication. See project board here: https://github.com/jacquelinegarrahan/pydm/projects/1?add_cards_query=is%3Aopen</p>"},{"location":"phoebus/","title":"Phoebus Alarm Server","text":"<p>For the translation of IOC alarm state to Kafka message and the handling of configuration and command representations, the new alarm system will use the CS-Studio Collaboration Phoebus alarm server. Alarm updates are configurable for receipt over Channel Access and pvAccess using either the \u201cca://\u201d or \u201cpva://\u201d prefix in the PV name configuration, respectively. Both may be used in a single configuration. </p> <p>The CS-Studio community has approved the following features for development:</p> <ul> <li>The configuration topic schema will be extended to include a variable number of miscellaneous tags</li> <li>The XML parsing model must be modified to accommodate nested inclusions</li> <li>The Alarm erver functionality must be extended to allow for the assignment of an expiration date for alarm bypasses. </li> </ul> <p>Documentation for the Phoebus Alarm Server may be found here.</p>"},{"location":"phoebus/#configuration-files","title":"Configuration files","text":"<p>Alarm configurations are XML files organized with component (group), and PV tags. Component tags accept specifications for guidance, display, commands, and automated actions. Configuration options for groups are defined below:</p> Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. <p>PV tags accept specifications for enabling, latching, annunciating, description, delay, commands, associated displays, guidance, alarm count, filter, and automated actions. A configuration schema is provided here.</p> Configuration tag Description guidance Message explaining the meaning of an alarm to the user and who to contact for resolution display Link to the associated control system display command Commands that may be invoked from user interfaces automated_action Action performed when a group enters and remains in an active alarm state. description Text displayed in the alarm table when the alarm is triggered delay Alarm will be triggered if the PV remains in alarm for at least this time enabled If false, ignore the value of this PV latching Alarms will latch to the highest severity until the alarm is acknowledged and cleared. If false, alarm may recover without requiring acknowledgment count If the trigger PV exhibits a not-OK alarm severity for more than \u2018count\u2019 times within the alarm delay, recognize the alarm filter An optional expression that can enable the alarm based on other PVs. <p></p>"},{"location":"phoebus/#phoebus-alarm-logger","title":"Phoebus Alarm Logger","text":"<p>The Phoebus alarm logger is used for the translation of Kafka messages to Elasticsearch. While multiple configurations may be grouped into a single logger, this consolidation may lead to logging outages in the event of configuration deprecation or configuration deployment. For this reason, it may be suitable to run a designated alarm logger instance per configuration.\u202f </p> <p></p> <p>Elasticsearch is a search engine build over the widely used Apache Lucene library, a Java-based search and indexing tool. Elasticsearch manages Lucene at scale, managing indices in a distributed fashion and providing additional data management and access features. JSON documents are written to an Elasticsearch server where they are tokenized, analyzed, and stored alongside indexed data representations of field values.</p> <p>Elasticsearch indices are created for the alarm events using following schemes, using the creation date of the index: <code>{config_name}_alarms_state_yyyy_mm_dd,  {config_name}_alarms_config_yyyy_mm_dd, and {config_name}_alarms_cmd_yyyy_mm_dd</code>. Indices are duration based using a default of one month. This may be reduced or extended based on volume. </p> <p>Elasticsearch supports aggregated metrics accessible via query including percentiles, summations, and averages. Custom expressions may be evaluated using the packaged scripting API, written in the painless language, or by building a custom Java plugin.</p> <p>Index aliasing may be a more effective way to handle the indices, with rollover of old indices automated, to avoid implicit datecoding of log names. More sophisticated elasticsearch lifecycle management should be explored and the X-pack enterprise option considered.</p> Index Field {config_name}_alarms_state_yyyy_mm_dd config (pv path with topic prefix) current_message current_severity latch message message_time mode notify pv severity time value {config_name}_alarms_config_yyyy_mm_dd config config_msg enabled host latch message user {config_name}_alarms_cmd_yyyy_mm_dd command config host message_time user"},{"location":"tools/","title":"Tools","text":""},{"location":"tools/#alarm-configuration-editor-tool","title":"Alarm Configuration Editor Tool","text":"<p>The alarm configuration editor is be a PyQt tool for designing the alarm configuration XML files for use with the Phoebus alarm server as outlined in Section 3.2. Alternatively, any XML editor may be used to build the document directly. The editor has the following features:\u202f  * Ability to edit alarm hierarchy, create new groups and new PVs  * Ability to define all configuration items * Optional conversion and import of legacy ALH files </p> <p>Requirements for running the editor are given in the <code>environment.yml</code> file bundled with the NALMS package. This environment can be created with conda using:</p> <pre><code>$ conda env create -f environment.yml\n</code></pre> <p>And subsequently activated:</p> <pre><code>$ conda activate nalms\n</code></pre> <p>If choosing to build your own environment without conda, the requirements follow:   - python =3.8   - treelib   - lxml   - pyqt5   - kafka-python   - pydm</p> <p>PyDM dependence will eventually be dropped.   </p> <p>To launch the editor run:</p> <pre><code>$ bash cli/nalms launch-editor\n</code></pre>"},{"location":"tools/#pydm-widgets","title":"PyDM widgets","text":"<p>PyDM widgets are in the development stage and relevant code is hosted in pydm-nalms. The integration of the datasource with PyDM is largely dependent upon the development of entrypoints for datasources. This feature request is still open and therefore intermediate use will require modifying PyDM directly or monkeypatching... </p>"},{"location":"tools/#nalms-tools","title":"NALMS-tools","text":""},{"location":"tools/#alh-conversion","title":"ALH Conversion","text":"<p>The ALH conversion tool is a python script for the conversion of legacy Alarm Handler Configuration tools. This script provides a command line interface for the purpose of translating an indicated Alarm Handler configuration file into a Phoebus alarm server -compatible XML configuration file, recursively iterating over its inclusions. Further, a report will be generated during execution describing the mapping of original to translated filenames and omissions of incompatible configuration elements. </p> <p>Of the configuration elements defined by the Alarm Handler, ALIAS, ACKPV, SEVRCOMMAND, STATCOMMAND, and BEEPSEVERITY have no analogs in the Phoebus alarm server.\u202f </p> <p>It is assumed that SEVRCOMMAND and STATCOMMAND will be handled inside the alarm system logic as justified in Section 3.2.1. BEEPSEVERITY, ACKPV, and ALIAS functionality will be deprecated. At SLAC, survey of the configuration utilization suggested that the LCLS makes no use of the STATCOMMAND and makes minimal use of the SEVRCOMMAND for sending emails in the case of high severity alarms. </p> <p>For use see legacy.</p>"}]}